{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic sentiment analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ac92JM1PrB",
        "outputId": "ca9d9316-37ff-47b8-aa0e-c45fafc4a445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s03gmma0K1J",
        "outputId": "a3effc28-a9c3-4901-ba0c-83471b42232d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID                                               Feed Sentiment\n",
            "0   1   اربد فيها جامعات اكثر من عمان ... وفيها قد عم...  Positive\n",
            "1   2   الحلو انكم بتحكوا على اساس انو الاردن ما فيه ...  Negative\n",
            "2   3                            كله رائع بجد ربنا يكرمك  Positive\n",
            "3   4                                 لسانك قذر يا قمامه  Negative\n",
            "4   5  ​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش...  Negative\n",
            "        ID                                               Feed Sentiment\n",
            "1185  1186                                كن مع الله لا تبالي  Positive\n",
            "197    198  الدين عباره عن اخلاق وحسن التعامل مع الناس وته...  Positive\n",
            "1395  1396  معايا زميل زميل يلعن المسخره كلهم مايستحون ذووولي  Negative\n",
            "120    121  اكيد طبعا حب ربنا احسن من حب اي حد وانا فعلا ع...  Positive\n",
            "1400  1401                                      معلومه ممتازه  Positive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_excel(r\"/content/AJGT.xlsx\")\n",
        "print(data.head())\n",
        "print(data.sample(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "\n",
        "# string.punctuation = !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
        "\n",
        "# Arabic stop words with nltk\n",
        "stop_words = stopwords.words()\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Shadda\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE) \n",
        "\n",
        "def preprocess(text):\n",
        "    \n",
        "    '''\n",
        "    text is an arabic string input\n",
        "    \n",
        "    the preprocessed text is returned\n",
        "    '''\n",
        "    \n",
        "    #remove punctuations\n",
        "    translator = str.maketrans('' ,  '', punctuations) # maps punctuations to none.\n",
        "\n",
        "    \n",
        "\n",
        "    text = text.translate(translator)\n",
        "    \n",
        "    \n",
        "    # remove Tashkeel\n",
        "    text = re.sub(arabic_diacritics, '', text) \n",
        "    \n",
        "    #remove longation\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "\n",
        "    \n",
        "\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return text\n",
        "  \n",
        "  \n",
        "data['Feed'] = data['Feed'].apply(preprocess)\n",
        "print(data.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPFK7WpR0x_r",
        "outputId": "19820bb6-d44d-4c6a-973d-01fef04528bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID                                               Feed Sentiment\n",
            "0   1  اربد جامعات اكثر عمان وفيها عمان ونص لعيبه الم...  Positive\n",
            "1   2   الحلو انكم بتحكوا علي اساس انو الاردن فساد سرقات  Negative\n",
            "2   3                            كله راءع بجد ربنا يكرمك  Positive\n",
            "3   4                                    لسانك قذر قمامه  Negative\n",
            "4   5  ​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش...  Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into target and feature\n",
        "feature = data.Feed # tweets\n",
        "target = data.Sentiment # {positive, negative, neutral}\n",
        "\n",
        "# splitting into train and tests\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =0.2)\n",
        "\n",
        "# the pipe will take care of transformations needs to be done on the output of\n",
        "# the vectorizer and gives it to the logistic regression model\n",
        "pipe = make_pipeline(TfidfVectorizer(),\n",
        "                   LogisticRegression())\n",
        "\n",
        "# TfidfVectorizer: helps us get the significant word (the rare one) \n",
        "# (tweet_no, word_index) --> how much is it significant. \n",
        "\n",
        "print(pipe)\n",
        "\n",
        "# param_grid for the hyperparamter c: regularization term.\n",
        "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Grid Search CV: searches in the parameter_grid to get the best one.\n",
        "model = GridSearchCV(pipe, param_grid)\n",
        "\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfcUQzkY1fML",
        "outputId": "6986ed9b-792c-418b-da0e-d5ea3b53de70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
            "                ('logisticregression', LogisticRegression())])\n",
            "Accuracy score is 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t3Il_KLeB9HV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}